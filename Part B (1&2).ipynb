{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0afecb92-0916-44b6-9924-a96813f22de3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType, StringType\n",
    "from pyspark.sql.functions import to_date, col, trim,concat,current_date, current_timestamp\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "path_insuranceinfo1=\"dbfs:/FileStore/hackathon/insuranceinfo1.csv\"\n",
    "path_insuranceinfo2=\"dbfs:/FileStore/hackathon/insuranceinfo1.csv\"\n",
    "path_custs_states=\"dbfs:/FileStore/hackathon/custs_states.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2aef8c60-4017-4774-8c62-c45ea598f1b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Part B - Spark DF & SQL#####\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42707135-399b-4205-b432-33ab16b62c11",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####1.\tDataFrames operations  (55% Completion) â€“ Total 55%####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d58ed40-d2f8-41d2-a306-0932c793b70f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Define Schemas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d61a18c4-bcc2-4586-bdb0-1ac5f78c3245",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.A.  StructType([StructField('IssuerId', IntegerType(), True), StructField('IssuerId2', IntegerType(), True), StructField('BusinessDate', DateType(), True), StructField('StateCode', StringType(), True), StructField('SourceName', StringType(), True), StructField('NetworkName', StringType(), True), StructField('NetworkURL', StringType(), True), StructField('custnum', StringType(), True), StructField('MarketCoverage', StringType(), True), StructField('DentalOnlyPlan', StringType(), True)])\n1.B.  StructType([StructField('IssuerId', IntegerType(), True), StructField('IssuerId2', IntegerType(), True), StructField('BusinessDate', StringType(), True), StructField('StateCode', StringType(), True), StructField('SourceName', StringType(), True), StructField('NetworkName', StringType(), True), StructField('NetworkURL', StringType(), True), StructField('custnum', StringType(), True), StructField('MarketCoverage', StringType(), True), StructField('DentalOnlyPlan', StringType(), True)])\n1.C.  StructType([StructField('IssuerId', IntegerType(), True), StructField('IssuerId2', IntegerType(), True), StructField('BusinessDate', StringType(), True), StructField('StateCode', StringType(), True), StructField('SourceName', StringType(), True), StructField('NetworkName', StringType(), True), StructField('NetworkURL', StringType(), True), StructField('custnum', StringType(), True), StructField('MarketCoverage', StringType(), True), StructField('DentalOnlyPlan', StringType(), True), StructField('RejectRows', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#1)\n",
    "insuranceinfo1_cust_schema = StructType([\n",
    "    StructField(\"IssuerId\", IntegerType()),\n",
    "    StructField(\"IssuerId2\", IntegerType()),\n",
    "    StructField(\"BusinessDate\", DateType()),\n",
    "    StructField(\"StateCode\", StringType()),\n",
    "    StructField(\"SourceName\", StringType()),\n",
    "    StructField(\"NetworkName\", StringType()),\n",
    "    StructField(\"NetworkURL\", StringType()),\n",
    "    StructField(\"custnum\", StringType()),\n",
    "    StructField(\"MarketCoverage\", StringType()),\n",
    "    StructField(\"DentalOnlyPlan\", StringType())\n",
    "])\n",
    "print(\"1.A. \",insuranceinfo1_cust_schema)\n",
    "insuranceinfo2_cust_schema_1 = StructType([\n",
    "    StructField(\"IssuerId\", IntegerType()),\n",
    "    StructField(\"IssuerId2\", IntegerType()),\n",
    "    StructField(\"BusinessDate\", StringType()),\n",
    "    StructField(\"StateCode\", StringType()),\n",
    "    StructField(\"SourceName\", StringType()),\n",
    "    StructField(\"NetworkName\", StringType()),\n",
    "    StructField(\"NetworkURL\", StringType()),\n",
    "    StructField(\"custnum\", StringType()),\n",
    "    StructField(\"MarketCoverage\", StringType()),\n",
    "    StructField(\"DentalOnlyPlan\", StringType())\n",
    "])\n",
    "print(\"1.B. \",insuranceinfo2_cust_schema_1)\n",
    "insuranceinfo2_cust_schema_2 = StructType([\n",
    "    StructField(\"IssuerId\", IntegerType()),\n",
    "    StructField(\"IssuerId2\", IntegerType()),\n",
    "    StructField(\"BusinessDate\", StringType()),\n",
    "    StructField(\"StateCode\", StringType()),\n",
    "    StructField(\"SourceName\", StringType()),\n",
    "    StructField(\"NetworkName\", StringType()),\n",
    "    StructField(\"NetworkURL\", StringType()),\n",
    "    StructField(\"custnum\", StringType()),\n",
    "    StructField(\"MarketCoverage\", StringType()),\n",
    "    StructField(\"DentalOnlyPlan\", StringType()),\n",
    "    StructField(\"RejectRows\", StringType())\n",
    "])\n",
    "print(\"1.C. \",insuranceinfo2_cust_schema_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5e5684d-6b71-4919-b14f-480d4c14421e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Load DataFrames with specified schemas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c48c3e2a-1247-4517-9b05-1d8bdca1714e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.\nDF with insuranceinfo1.csv \n+--------+---------+------------+---------+----------+--------------------+--------------------+-------+--------------+--------------+\n|IssuerId|IssuerId2|BusinessDate|StateCode|SourceName|         NetworkName|          NetworkURL|custnum|MarketCoverage|DentalOnlyPlan|\n+--------+---------+------------+---------+----------+--------------------+--------------------+-------+--------------+--------------+\n|   21989|    21989|  2019-10-01|       AK|      HIOS|         ODS Premier|https://www.modah...|     13|          null|          null|\n|   38344|    38344|  2019-10-01|       AK|      HIOS|        HeritagePlus|https://www.preme...|     13|          null|          null|\n|   38536|    38536|  2019-10-01|       AK|      HIOS|Lincoln Dental Co...|http://lfg.go2den...|     13|          null|          null|\n|   42507|    42507|  2019-10-01|       AK|      HIOS|DentalGuard Prefe...|https://www.guard...|     13|          null|          null|\n|   73836|    73836|  2019-10-01|       AK|      HIOS|Moda Plus AK Regi...|https://www.modah...|     13|          null|          null|\n+--------+---------+------------+---------+----------+--------------------+--------------------+-------+--------------+--------------+\nonly showing top 5 rows\n\nDF with insuranceinfo2.csv  \n+--------+---------+------------+---------+----------+--------------------+--------------------+-------+--------------+--------------+\n|IssuerId|IssuerId2|BusinessDate|StateCode|SourceName|         NetworkName|          NetworkURL|custnum|MarketCoverage|DentalOnlyPlan|\n+--------+---------+------------+---------+----------+--------------------+--------------------+-------+--------------+--------------+\n|   21989|    21989|        null|       AK|      HIOS|         ODS Premier|https://www.modah...|     13|          null|          null|\n|   38344|    38344|        null|       AK|      HIOS|        HeritagePlus|https://www.preme...|     13|          null|          null|\n|   38536|    38536|        null|       AK|      HIOS|Lincoln Dental Co...|http://lfg.go2den...|     13|          null|          null|\n|   42507|    42507|        null|       AK|      HIOS|DentalGuard Prefe...|https://www.guard...|     13|          null|          null|\n|   73836|    73836|        null|       AK|      HIOS|Moda Plus AK Regi...|https://www.modah...|     13|          null|          null|\n+--------+---------+------------+---------+----------+--------------------+--------------------+-------+--------------+--------------+\nonly showing top 5 rows\n\ndf3_with_corrupt with insuranceinfo2.csv with reject rows \n+--------+---------+------------+---------+----------+--------------------+--------------------+-------+--------------+--------------+--------------------+\n|IssuerId|IssuerId2|BusinessDate|StateCode|SourceName|         NetworkName|          NetworkURL|custnum|MarketCoverage|DentalOnlyPlan|          RejectRows|\n+--------+---------+------------+---------+----------+--------------------+--------------------+-------+--------------+--------------+--------------------+\n|   21989|    21989|  2019-10-01|       AK|      HIOS|         ODS Premier|https://www.modah...|     13|          null|          null|                null|\n|     219|      219|  2019-10-01|       AK|      HIOS|         ODS Premier|                null|   null|          null|          null|219,219,2019-10-0...|\n|   38344|    38344|  2019-10-01|       AK|      HIOS|        HeritagePlus|https://www.preme...|     13|          null|          null|                null|\n|   38536|    38536|  2019-10-01|       AK|      HIOS|Lincoln Dental Co...|http://lfg.go2den...|     13|          null|          null|                null|\n|   42507|    42507|  2019-10-01|       AK|      HIOS|DentalGuard Prefe...|https://www.guard...|     13|          null|          null|                null|\n+--------+---------+------------+---------+----------+--------------------+--------------------+-------+--------------+--------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-375575487821395>:30\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdf3_with_corrupt with insuranceinfo2.csv with reject rows \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     29\u001B[0m df3_with_corrupt\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n",
       "\u001B[0;32m---> 30\u001B[0m corrupt_df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/hackathon/insuranceinfo2_rejected_records1.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n",
       "\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n",
       "\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n",
       "\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n",
       "\u001B[1;32m   1797\u001B[0m )\n",
       "\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/FileStore/hackathon/insuranceinfo2_rejected_records1.csv already exists."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-375575487821395>:30\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdf3_with_corrupt with insuranceinfo2.csv with reject rows \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     29\u001B[0m df3_with_corrupt\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n\u001B[0;32m---> 30\u001B[0m corrupt_df\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/FileStore/hackathon/insuranceinfo2_rejected_records1.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1798\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m   1781\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m   1782\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1796\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[1;32m   1797\u001B[0m )\n\u001B[0;32m-> 1798\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Path dbfs:/FileStore/hackathon/insuranceinfo2_rejected_records1.csv already exists.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Path dbfs:/FileStore/hackathon/insuranceinfo2_rejected_records1.csv already exists.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2)\n",
    "df1=spark.read.csv(path_insuranceinfo1,schema=insuranceinfo1_cust_schema,header=True,mode=\"dropmalformed\")\n",
    "#df1.printSchema()\n",
    "print(\"2.\")\n",
    "print(\"DF with insuranceinfo1.csv \")\n",
    "df1.show(5)\n",
    "\n",
    "df2=spark.read.csv(path_insuranceinfo2,schema=insuranceinfo2_cust_schema_1,header=True,mode=\"dropmalformed\")\n",
    "df2 = df2.withColumn(\"BusinessDate\", to_date(df2[\"BusinessDate\"], \"dd-MM-yyyy\"))\n",
    "print(\"DF with insuranceinfo2.csv  \")\n",
    "df2.show(5)\n",
    "\n",
    "#df3_with_corrupt=spark.read.csv(\"file:///home/hduser/sparkhack2/insuranceinfo2.csv\",schema=insuranceinfo2_cust_schema_2,header=True,mode=\"permissive\",columnNameOfCorruptRecord=\"RejectRows\",ignoreLeadingWhiteSpace=True,ignoreTrailingWhiteSpace=True)\n",
    "df3_with_corrupt = (spark.read\n",
    "                    .format(\"csv\")\n",
    "                    .option(\"header\", \"true\")\n",
    "                    .option(\"mode\", \"permissive\")\n",
    "                    .option(\"columnNameOfCorruptRecord\", \"RejectRows\")\n",
    "                    .option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "                    .option(\"ignoreTrailingWhiteSpace\", \"true\")\n",
    "                    .schema(insuranceinfo2_cust_schema_2)\n",
    "                    .load(path_insuranceinfo2))\n",
    "\n",
    "#df3_with_corrupt.printSchema()\n",
    "df3_with_corrupt.cache()\n",
    "corrupt_df=df3_with_corrupt.where(\"RejectRows is not null\")\n",
    "#df3_with_corrupt.select('RejectRows').show()\n",
    "print(\"df3_with_corrupt with insuranceinfo2.csv with reject rows \")\n",
    "df3_with_corrupt.show(5)\n",
    "corrupt_df.write.option(\"header\", \"true\").csv(\"dbfs:/FileStore/hackathon/insuranceinfo2_rejected_records1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45207c7a-00de-4c6f-8265-da816c8c35f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Merge and Transform DataFrames**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "749a1dad-e02d-443e-af3e-e5bacce533bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.a.merged_df with colums renamed : StateCode and SourceName as stcd and srcnm \n+--------+---------+------------+----+-----+--------------------+--------------------+-------+--------------+--------------+\n|IssuerId|IssuerId2|BusinessDate|stcd|srcnm|         NetworkName|          NetworkURL|custnum|MarketCoverage|DentalOnlyPlan|\n+--------+---------+------------+----+-----+--------------------+--------------------+-------+--------------+--------------+\n|   21989|    21989|  2019-10-01|  AK| HIOS|         ODS Premier|https://www.modah...|     13|          null|          null|\n|   38344|    38344|  2019-10-01|  AK| HIOS|        HeritagePlus|https://www.preme...|     13|          null|          null|\n|   38536|    38536|  2019-10-01|  AK| HIOS|Lincoln Dental Co...|http://lfg.go2den...|     13|          null|          null|\n|   42507|    42507|  2019-10-01|  AK| HIOS|DentalGuard Prefe...|https://www.guard...|     13|          null|          null|\n|   73836|    73836|  2019-10-01|  AK| HIOS|Moda Plus AK Regi...|https://www.modah...|     13|          null|          null|\n+--------+---------+------------+----+-----+--------------------+--------------------+-------+--------------+--------------+\nonly showing top 5 rows\n\n3.b.merged_df with Concat IssuerId,IssuerId2 as issueridcomposite and make it as a new field  \n+--------+---------+------------+----+-----+--------------------+--------------------+-------+--------------+--------------+-----------------+\n|IssuerId|IssuerId2|BusinessDate|stcd|srcnm|         NetworkName|          NetworkURL|custnum|MarketCoverage|DentalOnlyPlan|issueridcomposite|\n+--------+---------+------------+----+-----+--------------------+--------------------+-------+--------------+--------------+-----------------+\n|   21989|    21989|  2019-10-01|  AK| HIOS|         ODS Premier|https://www.modah...|     13|          null|          null|       2198921989|\n|   38344|    38344|  2019-10-01|  AK| HIOS|        HeritagePlus|https://www.preme...|     13|          null|          null|       3834438344|\n|   38536|    38536|  2019-10-01|  AK| HIOS|Lincoln Dental Co...|http://lfg.go2den...|     13|          null|          null|       3853638536|\n|   42507|    42507|  2019-10-01|  AK| HIOS|DentalGuard Prefe...|https://www.guard...|     13|          null|          null|       4250742507|\n|   73836|    73836|  2019-10-01|  AK| HIOS|Moda Plus AK Regi...|https://www.modah...|     13|          null|          null|       7383673836|\n+--------+---------+------------+----+-----+--------------------+--------------------+-------+--------------+--------------+-----------------+\nonly showing top 5 rows\n\n3.c.merged_df with Remove DentalOnlyPlan column\n+--------+---------+------------+----+-----+--------------------+--------------------+-------+--------------+-----------------+\n|IssuerId|IssuerId2|BusinessDate|stcd|srcnm|         NetworkName|          NetworkURL|custnum|MarketCoverage|issueridcomposite|\n+--------+---------+------------+----+-----+--------------------+--------------------+-------+--------------+-----------------+\n|   21989|    21989|  2019-10-01|  AK| HIOS|         ODS Premier|https://www.modah...|     13|          null|       2198921989|\n|     219|      219|  2019-10-01|  AK| HIOS|         ODS Premier|                null|   null|          null|           219219|\n|   38344|    38344|  2019-10-01|  AK| HIOS|        HeritagePlus|https://www.preme...|     13|          null|       3834438344|\n|   38536|    38536|  2019-10-01|  AK| HIOS|Lincoln Dental Co...|http://lfg.go2den...|     13|          null|       3853638536|\n|   42507|    42507|  2019-10-01|  AK| HIOS|DentalGuard Prefe...|https://www.guard...|     13|          null|       4250742507|\n+--------+---------+------------+----+-----+--------------------+--------------------+-------+--------------+-----------------+\nonly showing top 5 rows\n\n3.d.merged_df with added columns : sys date and timestamp\n+--------+---------+------------+----+-----+--------------------+--------------------+-------+--------------+-----------------+----------+--------------------+\n|IssuerId|IssuerId2|BusinessDate|stcd|srcnm|         NetworkName|          NetworkURL|custnum|MarketCoverage|issueridcomposite|     sysdt|               systs|\n+--------+---------+------------+----+-----+--------------------+--------------------+-------+--------------+-----------------+----------+--------------------+\n|   21989|    21989|  2019-10-01|  AK| HIOS|         ODS Premier|https://www.modah...|     13|          null|       2198921989|2024-07-21|2024-07-21 15:12:...|\n|     219|      219|  2019-10-01|  AK| HIOS|         ODS Premier|                null|   null|          null|           219219|2024-07-21|2024-07-21 15:12:...|\n|   38344|    38344|  2019-10-01|  AK| HIOS|        HeritagePlus|https://www.preme...|     13|          null|       3834438344|2024-07-21|2024-07-21 15:12:...|\n|   38536|    38536|  2019-10-01|  AK| HIOS|Lincoln Dental Co...|http://lfg.go2den...|     13|          null|       3853638536|2024-07-21|2024-07-21 15:12:...|\n|   42507|    42507|  2019-10-01|  AK| HIOS|DentalGuard Prefe...|https://www.guard...|     13|          null|       4250742507|2024-07-21|2024-07-21 15:12:...|\n+--------+---------+------------+----+-----+--------------------+--------------------+-------+--------------+-----------------+----------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "#3)\n",
    "merged_df = df1.union(df2)\n",
    "#print(df1.count(),df2.count(),merged_df.count())\n",
    "#3.a Rename Column\n",
    "merged_df=merged_df.withColumnRenamed(\"StateCode\",\"stcd\").withColumnRenamed(\"SourceName\",\"srcnm\")\n",
    "print(\"3.a.merged_df with colums renamed : StateCode and SourceName as stcd and srcnm \")\n",
    "merged_df.show(5)\n",
    "#3.b Concat Column\n",
    "merged_df=merged_df.withColumn(\"issueridcomposite\",concat(col(\"IssuerId\").cast(StringType()), col(\"IssuerId2\").cast(StringType())))\n",
    "print(\"3.b.merged_df with Concat IssuerId,IssuerId2 as issueridcomposite and make it as a new field  \")\n",
    "merged_df.show(5)\n",
    "#3.c remove Column\n",
    "merged_df=merged_df.drop(\"DentalOnlyPlan\")\n",
    "print(\"3.c.merged_df with Remove DentalOnlyPlan column\")\n",
    "merged_df.show(5)\n",
    "#3.D add columns\n",
    "merged_df = merged_df.withColumn(\"sysdt\", current_date())\n",
    "merged_df = merged_df.withColumn(\"systs\", current_timestamp())\n",
    "print(\"3.d.merged_df with added columns : sys date and timestamp\")\n",
    "merged_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49b25316-c6f9-4850-81c0-d50a63f17256",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Interesting Usecases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24adca66-cea8-4771-beff-1c35488f20c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i.All the column names stored in an List variable \n['IssuerId', 'IssuerId2', 'BusinessDate', 'stcd', 'srcnm', 'NetworkName', 'NetworkURL', 'custnum', 'MarketCoverage', 'issueridcomposite', 'sysdt', 'systs']\nii.all columns with it's datatype stored in a list variable \n[('IssuerId', 'int'), ('IssuerId2', 'int'), ('BusinessDate', 'date'), ('stcd', 'string'), ('srcnm', 'string'), ('NetworkName', 'string'), ('NetworkURL', 'string'), ('custnum', 'string'), ('MarketCoverage', 'string'), ('issueridcomposite', 'string'), ('sysdt', 'date'), ('systs', 'timestamp')]\niii.all integer columns stored in an list  \n['IssuerId', 'IssuerId2']\niii.iv.\tSelect only the integer columns and display 10 records  \n+--------+---------+\n|IssuerId|IssuerId2|\n+--------+---------+\n|   21989|    21989|\n|     219|      219|\n|   38344|    38344|\n|   38536|    38536|\n|   42507|    42507|\n|   73836|    73836|\n|   73836|    73836|\n|   74819|    74819|\n|   84859|    84859|\n|   12538|    12538|\n+--------+---------+\nonly showing top 10 rows\n\nv.additional column in the reject dataframe subtracting the columns between df1 and df3 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#i.\n",
    "merged_df_collist=merged_df.columns\n",
    "print(\"i.All the column names stored in an List variable \")\n",
    "print(merged_df_collist)\n",
    "#print(merged_df_collist)\n",
    "#ii.\n",
    "merged_df_dtypelist=merged_df.dtypes\n",
    "print(\"ii.all columns with it's datatype stored in a list variable \")\n",
    "print(merged_df_dtypelist)\n",
    "#iii.\n",
    "integer_columns = [col_name for col_name, dtype in merged_df_dtypelist if dtype == 'int']\n",
    "print(\"iii.all integer columns stored in an list  \")\n",
    "print(integer_columns)\n",
    "#iv.\n",
    "df_integer = merged_df.select(integer_columns)\n",
    "print(\"iii.iv.\tSelect only the integer columns and display 10 records  \")\n",
    "df_integer.show(10)\n",
    "#v.\n",
    "columns_df1 = set(df1.columns)\n",
    "columns_df3 = set(df3_with_corrupt.columns)\n",
    "additional_columns = columns_df3 - columns_df1\n",
    "print(\"v.additional column in the reject dataframe subtracting the columns between df1 and df3 \")\n",
    "#print(additional_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3aa1c230-2564-41e6-99f2-11767a9ff10d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Remove Nulls and Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bd28808-ddf8-4cab-ae00-798b9fbc396d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.Remove the rows contains null in any one of the field \n  and count the number of rows which contains all columns with some value\nNumber of rows with no null values: 1408\n"
     ]
    }
   ],
   "source": [
    "\n",
    "merged_df_cleaned = merged_df.dropna()\n",
    "clean_row_count = merged_df_cleaned.count()\n",
    "print(\"4.Remove the rows contains null in any one of the field\",'\\n',\" and count the number of rows which contains all columns with some value\")\n",
    "print(\"Number of rows with no null values:\", clean_row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f52bb55-3e13-4cf3-aab5-7f35b5bb6037",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Custom UDF and String Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e4912d8-03c5-4e14-a80e-665ef4067620",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./CustomPackage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f10473-6579-4d64-ac26-b4ef1173b69f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 and 6. Custom method creation in package and import package for cleaning string using regexp\n+---+----------------------------+-------------------------+\n|id |string                      |cleaned_string           |\n+---+----------------------------+-------------------------+\n|1  |Pathway - 2X (with dental)  |Pathway X with dental    |\n|2  |Pathway @ 4X [with braces]  |Pathway @ X [with braces]|\n|3  |Pathway - 5X (with check-up)|Pathway X with check-up  |\n+---+----------------------------+-------------------------+\n\n7.Using imported custom package to clean Network Name column in DF\n+----------------------------------+--------------------------------+\n|NetworkName                       |Cleaned_NetworkName             |\n+----------------------------------+--------------------------------+\n|EHB Basic Dental Plan (Low)       |EHB Basic Dental Plan Low       |\n|EHB Enhanced Dental Plan (High)   |EHB Enhanced Dental Plan High   |\n|Family Basic Dental Plan (Low)    |Family Basic Dental Plan Low    |\n|Family Enhanced Dental Plan (High)|Family Enhanced Dental Plan High|\n|Total Dental Administrators (TDA) |Total Dental Administrators TDA |\n|EHB Basic Dental Plan (Low)       |EHB Basic Dental Plan Low       |\n|EHB Enhanced Dental Plan (High)   |EHB Enhanced Dental Plan High   |\n|Family Basic Dental Plan (Low)    |Family Basic Dental Plan Low    |\n|Family Enhanced Dental Plan (High)|Family Enhanced Dental Plan High|\n|EHB Basic Dental Plan (Low)       |EHB Basic Dental Plan Low       |\n+----------------------------------+--------------------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "#5&6&7\n",
    "\n",
    "\n",
    "remspecialchar_udf = udf(remspecialchar, StringType())\n",
    "\n",
    "data = [\n",
    "    (1, \"Pathway - 2X (with dental)\"),\n",
    "    (2, \"Pathway @ 4X [with braces]\"),\n",
    "    (3, \"Pathway - 5X (with check-up)\"),\n",
    "]\n",
    "schema = [\"id\", \"string\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df_with_cleaned_str = df.withColumn(\"cleaned_string\", remspecialchar_udf(df[\"string\"]))\n",
    "print(\"5 and 6. Custom method creation in package and import package for cleaning string using regexp\")\n",
    "df_with_cleaned_str.show(truncate=False)\n",
    "\n",
    "#7.\n",
    "df1_with_cleaned_networkname=df1.withColumn(\"Cleaned_NetworkName\",remspecialchar_udf(df1[\"NetworkName\"]))\n",
    "print(\"7.Using imported custom package to clean Network Name column in DF\" )\n",
    "df1_with_cleaned_networkname.select('NetworkName','Cleaned_NetworkName').\\\n",
    "    where(\"NetworkName like '%(%' \")\\\n",
    "    .show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c92a5fd-43bd-49d2-99ac-c35b12cd057c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Resuable Write DataFrame to Files Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "213b7a26-336b-4645-90e6-f558067c72d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8. JSON of df1_with_cleaned_networkname stored in /FileStore/hackathon/df1_with_cleaned_networkname.json\n8. CSV of df1_with_cleaned_networkname stored in /FileStore/hackathon/df1_with_cleaned_networkname.csv\n"
     ]
    }
   ],
   "source": [
    "#8.\n",
    "#df1_with_cleaned_networkname.write.mode('overwrite').json(\"dbfs:/FileStore/hackathon/df1_with_cleaned_networkname.json\")\n",
    "print(\"8. JSON of df1_with_cleaned_networkname stored in /FileStore/hackathon/df1_with_cleaned_networkname.json\")\n",
    "#9.\n",
    "#df1_with_cleaned_networkname.write.option(\"header\", \"true\").option(\"delimiter\", \"~\").mode(\"overwrite\").csv(\"dbfs:/FileStore/hackathon/df1_with_cleaned_networkname.csv\")\n",
    "print(\"8. CSV of df1_with_cleaned_networkname stored in /FileStore/hackathon/df1_with_cleaned_networkname.csv\")\n",
    "#9.Note\n",
    "def writeToFile(sparkSession, df, fileType, location, delimiter=None, mode='overwrite'):\n",
    "\n",
    "    if fileType == 'json':\n",
    "        df.write.mode(mode).json(location)\n",
    "    elif fileType == 'csv':\n",
    "        df.write.option(\"header\", \"true\") \\\n",
    "            .option(\"delimiter\", delimiter) \\\n",
    "            .mode(mode) \\\n",
    "            .csv(location)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {fileType}\")\n",
    "\n",
    "writeToFile(spark, df1_with_cleaned_networkname, 'json', 'dbfs:/FileStore/hackathon/json/')\n",
    "\n",
    "# Save the DataFrame in CSV format with a custom delimiter\n",
    "writeToFile(spark, df1_with_cleaned_networkname, 'csv', 'dbfs:/FileStore/hackathon/csv/', delimiter='~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b5f6e18-b58b-47d6-8319-617c92f3faed",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Write DataFrame to Hive Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0ac0a5d-9506-482c-8788-d6a66d31af86",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.Write DF to Hive table\n"
     ]
    }
   ],
   "source": [
    "#10.\n",
    "print(\"10.Write DF to Hive table\")\n",
    "df1_with_cleaned_networkname.write.mode(\"overwrite\").saveAsTable(\"default.df1_with_cleaned_networkname\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caeb0ac3-4783-4592-834b-aa15e6683198",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####2.\tTale of handling RDDs, DFs and TempViews  (20% Completion) â€“ Total 75%####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7627e32c-3249-4101-8c0f-a28a619c7d42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "****Use RDD Functions****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd2c4007-5040-4d9c-a45f-d82f125ab8c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.Load the file3 (custs_states.csv) from the DBFS location using RDD\n['1,Kristina,Chung,55,Pilot', '2,Paige,Chen,74,Teacher', 'NM,New Mexico', 'NY,New York', 'NC,North Carolina', 'ND,North Dakota', 'OH,Ohio', 'OK,Oklahoma', 'OR,Oregon', 'MD,Maryland', 'MA,Massachusetts', 'MI,Michigan', 'MN,Minnesota', 'MS,Mississippi', 'MO,Missouri', 'PA,Pennsylvania', 'RI,Rhode Island', 'SC,South Carolina', 'SD,South Dakota', 'TN,Tennessee', 'TX,Texas', 'UT,Utah', 'VT,Vermont', 'VA,Virginia', 'WA,Washington', 'WV,West Virginia', 'WI,Wisconsin', 'WY,Wyoming', '3,Sherri,Melton,34,Firefighter', '4,Gretchen,Hill,66,Computer hardware engineer', '5,Karen,Puckett,74,Lawyer', '6,Patrick,Song,42,Veterinarian', '7,Elsie,Hamilton,43,Pilot', '8,Hazel,Bender,63,Carpenter', '9,Malcolm,Wagner,39,Artist', '10,Dolores,McLaughlin,60,Writer', '11,Francis,McNamara,47,Therapist', '12,Sandy,Raynor,26,Writer', '13,Marion,Moon,41,Carpenter', '14,Beth,Woodard,65,', '15,Julia,Desai,49,Musician', '16,Jerome,Wallace,52,Pharmacist', '17,Neal,Lawrence,72,Computer support specialist', '18,Jean,Griffin,45,Childcare worker', '19,Kristine,Dougherty,63,Financial analyst', '20,Crystal,Powers,67,Engineering technician', '21,Alex,May,39,Environmental scientist', '22,Eric,Steele,66,Doctor', '23,Wesley,Teague,42,Carpenter', '24,Franklin,Vick,28,Dancer', '25,Claire,Gallagher,42,Musician', '26,Marian,Solomon,27,Lawyer', '27,Marcia,Walsh,64,Accountant', '28,Dwight,Monroe,45,Economist', '29,Wayne,Connolly,40,Real estate agent', '30,Stephanie,Hawkins,50,Human resources assistant', '31,Neal,Middleton,59,Civil engineer', '32,Gretchen,Goldstein,24,Engineering technician', '33,Tim,Watts,58,Lawyer', '34,Jerome,Johnston,38,Childcare worker', '35,Shelley,Weeks,25,Reporter', '36,Priscilla,Wilkerson,35,Agricultural and food scientist', '37,Elsie,Barton,27,Childcare worker', '38,Beth,Walton,73,Firefighter', '39,Erica,Hall,33,Police officer', '40,Douglas,Ross,27,Secretary', '41,Donald,Chung,65,Computer hardware engineer', '42,Katherine,Bender,44,Physicist', '43,Paul,Woods,63,Doctor', '44,Patricia,Mangum,67,Civil engineer', '45,Lois,Joseph,44,Musician', '46,Louis,Rosenthal,31,', '47,Christina,Bowden,47,Computer software engineer', '48,Darlene,Barton,54,Doctor', '49,Harvey,Underwood,70,Engineering technician', '50,William,Jones,53,Photographer', '51,Frederick,Baker,52,Writer', '52,Shirley,Merritt,21,Reporter', '53,Jason,Cross,56,Civil engineer', '54,Judith,Cooper,22,Economist', '55,Gretchen,Holmes,39,Childcare worker', '56,Don,Sharpe,53,Social worker', '57,Glenda,Morgan,37,Real estate agent', '58,Scott,Hoyle,40,Doctor', '59,Pat,Allen,45,Secretary', '60,Michelle,Rich,44,Artist', '61,Jessica,Rich,37,Actor', '62,Evan,Grant,66,Agricultural and food scientist', '63,Melinda,Proctor,27,Teacher', '64,Calvin,Diaz,65,Athlete', '65,Eugene,Graham,52,Police officer', '66,Vickie,Watkins,55,Computer support specialist', '67,Luis,Hinton,69,Childcare worker', '68,Allan,Marsh,67,Athlete', '69,Melanie,Hewitt,47,Real estate agent', '70,Marianne,Branch,53,Judge', '71,Natalie,Walton,24,Recreation and fitness worker', \"72,Caroline,O'Brien,44,Computer support specialist\", '73,Arlene,Case,62,Musician', '74,Kyle,Watts,39,Engineering technician', '75,Calvin,Christensen,54,Architect', '76,Gary,Parks,65,Pharmacist', '77,Samantha,Hardin,27,Doctor', '78,Sara,Lucas,44,Loan officer', '79,Stacy,Eason,31,Musician', '80,Gladys,Davidson,52,Recreation and fitness worker', '81,Mike,Whitehead,26,Politician', '82,Lynne,Rose,36,Loan officer', '83,Faye,Sparks,61,Civil engineer', '84,Diana,Moore,44,Computer support specialist', '85,Leon,Pearson,24,Physicist', '86,Ethel,Rodgers,30,Librarian', '87,Steve,Graves,73,Nurse', '88,Alison,Scarborough,66,Designer', '89,Sherri,Sutton,75,Social worker', '90,Patsy,Sinclair,48,Police officer', '91,Kelly,Bowman,69,Childcare worker', '92,Stacy,Olsen,25,Veterinarian', '93,Curtis,Love,45,Secretary', '94,Dana,McLean,61,Artist', '95,Jennifer,Christian,54,Human resources assistant', '96,Brett,Lamb,39,Engineering technician', '97,Brandon,James,29,Musician', '98,Keith,Chandler,25,Coach', '99,Joann,Stout,32,Real estate agent', '100,Ronnie,Cowan,71,Photographer', '101,Scott,Golden,27,Teacher', '102,Gene,Bowling,73,Recreation and fitness worker', '103,Louise,Beasley,54,Loan officer', '104,Geoffrey,Clapp,41,Photographer', '105,Patricia,Abrams,51,Veterinarian', '106,Jennifer,Tilley,35,Agricultural and food scientist', '107,Mary,Morse,70,Automotive mechanic', '108,Shawn,Boykin,34,Photographer', '109,Vincent,Sumner,31,Lawyer', '110,Kurt,Cassidy,32,Dancer', '111,Danny,Davidson,70,Agricultural and food scientist', '112,Charlene,Heath,26,Electrician', '113,Alice,Blanchard,73,Economist', '114,Joan,McAllister,73,Engineering technician', '115,Betty,McKenzie,45,Computer support specialist', '116,Danny,Byrne,51,Dancer', '117,Peggy,Schroeder,61,Loan officer', '118,Leslie,Griffin,37,Photographer', '119,Marshall,Gross,42,Actor', '120,Sara,Perkins,67,Actor', '121,Martha,Robertson,37,Agricultural and food scientist', '122,Jack,Palmer,52,Human resources assistant', '123,Gayle,Brady,45,Firefighter', '124,Benjamin,Rowe,64,Childcare worker', '125,Roberta,Zhang,38,Statistician', '126,Patricia,Hodge,59,Artist', '127,Clifford,Li,63,Photographer', '128,Joanne,Bowling,35,Musician', '129,Martin,Justice,58,Electrician', '130,Toni,Glass,46,Lawyer', '131,Beth,Willis,62,Carpenter', '132,Jessica,Hester,72,Civil engineer', '133,Samantha,Floyd,72,Childcare worker', '134,Jimmy,Graves,69,Nurse', '135,Vincent,Fischer,33,Statistician', '136,Dianne,Norman,68,Veterinarian', '137,Rhonda,Chan,34,Pharmacist', '138,Tamara,Hunt,25,Psychologist', '139,Mary,Byrd,34,Environmental scientist', '140,Sidney,Lane,54,Statistician', '141,Jeff,Kaplan,70,Chemist', '142,Sandra,Heller,51,Photographer', '143,Katie,May,28,Recreation and fitness worker', '144,Raymond,Jennings,63,Coach', '145,Roger,Hanna,23,Musician', '146,Natalie,Locklear,37,Politician', '147,Kathy,Holloway,65,Pharmacist', '148,Troy,Jones,74,Secretary', '149,Neal,Glover,40,Real estate agent', '150,Martin,Vick,31,Physicist', \"151,Jack,O'Donnell,59,Actor\", '152,Vincent,Goldman,22,Electrician', '153,Beth,McKenna,40,Veterinarian', '154,Milton,Starr,23,Carpenter', '155,Tamara,Stone,73,Firefighter', '156,Mitchell,McClure,68,Loan officer', '157,Franklin,Watson,64,Coach', '158,Leroy,Monroe,51,Computer support specialist', '159,Glen,Abbott,29,Loan officer', '160,Judith,Singer,73,Actor', '161,Alice,Hall,33,Recreation and fitness worker', '162,Bruce,Farrell,35,Librarian', '163,Kathleen,Lucas,66,Chemist', '164,Amy,Norman,62,Automotive mechanic', '165,Ronnie,Atkins,67,Dancer', '166,Martha,Monroe,31,Judge', '167,Lynn,Robertson,45,Lawyer', '168,Jose,Sykes,47,Writer', '169,Robert,Reid,39,Carpenter', '170,Pauline,Chandler,70,Economist', '171,Stephen,Finch,30,Coach', '172,Peggy,Hobbs,69,Musician', '173,Donna,Adkins,72,Electrical engineer', '174,Doris,Kinney,36,Athlete', '175,Ben,Whitaker,35,Computer support specialist', '176,Kristin,Alexander,21,Coach', '177,Ryan,Conner,51,Electrical engineer', '178,Tracey,Waters,73,Computer hardware engineer', '179,Mark,Becker,39,Computer support specialist', '180,Louis,Rollins,35,Economist', '181,Janet,Love,39,Politician', '182,Leo,Adkins,42,Economist', '183,Constance,Black,36,Firefighter', '184,Sarah,Fox,73,Psychologist', '185,Gladys,Hatcher,62,Musician', '186,Hazel,Wu,68,Therapist', '187,Hazel,Lloyd,48,Politician', '188,Jerome,Joyce,40,Artist', '189,Vincent,Welch,54,Psychologist', '190,Kim,Matthews,38,Architect', '191,Joseph,Chappell,59,Reporter', '192,Alison,MacDonald,26,Childcare worker', '193,Eric,Kane,61,Childcare worker', '194,Heather,Butler,67,Farmer', '195,Claire,Pickett,59,Lawyer', '196,Michele,Bowman,54,Computer software engineer', '197,Terry,Barton,27,Recreation and fitness worker', '198,Ken,Kennedy,35,Automotive mechanic', '199,Judy,Branch,42,Athlete', '200,Clyde,Thornton,70,Pharmacist', '201,Timothy,McNeill,70,Librarian', '202,Ted,Weinstein,23,Human resources assistant', '203,Sandra,Middleton,28,Librarian', '204,Billie,Moss,65,Photographer', '205,Katharine,Lucas,67,Electrician', '206,Lester,Rich,53,Human resources assistant', '207,Johnny,Carlton,28,Librarian', '208,Ashley,Brady,43,Financial analyst', '209,Lauren,Schultz,33,Electrician', '210,Carolyn,Nichols,49,Agricultural and food scientist', '211,Louis,Harvey,72,Financial analyst', '212,Charlene,Stevenson,65,Carpenter', '213,Ashley,Houston,24,Judge', '214,Sheryl,Dunn,71,Civil engineer', '215,Ben,West,64,Electrician', \"216,Linda,O'Brien,46,Loan officer\", '217,Sarah,Barr,52,Therapist', '218,Brett,Snyder,22,Doctor', '219,Dana,Cain,63,Human resources assistant', '220,Rebecca,Heath,74,Environmental scientist', '221,Glenda,Boswell,28,Civil engineer', '222,Glen,Olsen,63,Childcare worker', '223,Don,Pittman,70,Designer', '224,Gregory,Weiner,58,Accountant', '225,Randy,Petersen,71,Actor', '226,Geraldine,Davis,36,Accountant', '227,Edna,Coleman,58,Veterinarian', '228,Sidney,Terrell,32,Statistician', '229,Faye,Norman,64,Pilot', '230,Kathy,Burch,28,Pilot', '231,Marguerite,Weiner,45,Police officer', '232,Marvin,Parrott,67,Doctor', '233,Alex,Henry,67,Police officer', '234,Alexander,Gray,35,Reporter', '235,Marsha,Chang,42,Chemist', '236,Karl,McLean,69,Photographer', '237,Laura,Eason,31,Loan officer', '238,Melinda,Weeks,59,Computer software engineer', '239,Eva,Siegel,50,Psychologist', '240,Robert,Puckett,56,Physicist', '241,Bill,Heath,35,Actor', '242,Edna,Hoyle,74,Electrical engineer', '243,Clifford,Garrett,72,Veterinarian', '244,Penny,Neal,59,Veterinarian', '245,Glenda,Baker,52,Economist', '246,Arthur,Goldman,58,Childcare worker', '247,Wesley,Shaffer,58,Farmer', '248,Allison,Choi,23,Computer hardware engineer', '249,Florence,Carver,34,Electrical engineer', '250,Claire,Shelton,34,Environmental scientist', '251,Jeremy,House,61,Pilot', '252,Christy,Lyons,31,Physicist', '253,Wesley,Moser,64,Doctor', '254,Lucy,Dickinson,72,Loan officer', '255,Susan,Abbott,65,Actor', '256,Nancy,Hobbs,40,Environmental scientist', '257,Monica,Dodson,58,', '258,Justin,Spencer,67,Economist', '259,Margaret,Burgess,74,Social worker', '260,Philip,Liu,59,Nurse', '261,Miriam,Wong,27,Childcare worker', '262,Miriam,Blackburn,52,Athlete', '263,Christopher,McKay,67,Judge', '264,Joanna,Middleton,37,Firefighter', '265,Stacy,Frazier,69,Designer', '266,Janice,Reid,40,Therapist', '267,Brian,Braswell,64,Chemist', '268,Juan,Steele,57,Nurse', '269,Leroy,Donovan,74,Photographer', '270,Tommy,Barrett,38,Engineering technician', '271,Alice,Nance,59,Pilot', '272,Adam,Washington,54,Electrical engineer', '273,Larry,Rogers,50,Engineering technician', '274,Angela,McMahon,66,Secretary', '275,Lawrence,Miles,74,Dancer', '276,Glen,Kramer,43,Coach', '277,Valerie,Jennings,59,Loan officer', '278,Meredith,Bowles,62,Accountant', '279,Jan,Brown,58,Secretary', '280,Lillian,Bolton,32,Photographer', '281,Allen,Craven,64,Therapist', '282,Ashley,Hendrix,50,Accountant', '283,Ian,Nichols,57,Electrician', '284,Gina,Saunders,48,Photographer', '285,Donna,Lehman,71,Social worker', '286,Catherine,Sherrill,48,Nurse', '287,Lester,Cash,30,Firefighter', '288,Jacob,Pittman,34,Economist', '289,Nancy,Sullivan,70,Electrical engineer', '290,Gail,Whitehead,21,Nurse', '291,Lloyd,Mack,67,Recreation and fitness worker', '292,Donna,Rice,21,Social worker', '293,Kay,Ayers,44,Childcare worker', '294,Wendy,Cherry,70,Loan officer', '295,Lucille,Richmond,70,Photographer', '296,Virginia,York,67,Politician', '297,Bruce,Wiley,43,Artist', '298,Marc,Harrington,57,Physicist', '299,Geoffrey,Reed,53,Reporter', '300,Tiffany,Nash,41,Coach', '301,Todd,Wilkerson,55,Librarian', '302,Jill,Kent,27,Electrical engineer', '303,Erin,Finch,54,Police officer', '304,Melinda,Starr,57,Statistician', '305,Julie,Holland,61,Architect', '306,Stanley,Glover,44,Nurse', '307,Karen,Clements,74,Farmer', '308,Judy,Schultz,42,Statistician', '309,Raymond,Hawley,66,Recreation and fitness worker', '310,Julie,Skinner,56,Electrical engineer', '311,Sue,Hamrick,43,Reporter', '312,Colleen,Winters,36,Designer', '313,Judith,Dolan,44,Civil engineer', '314,Luis,Turner,69,Computer software engineer', '315,Bradley,Beatty,36,Human resources assistant', '316,Glenda,Douglas,28,Loan officer', '317,Brent,Byrne,62,Judge', '318,Clyde,Hendricks,41,Loan officer', '319,Sharon,Mayer,70,Statistician', '320,Mary,Cochran,25,Computer support specialist', '321,Sue,Reilly,33,Physicist', '322,Geraldine,Jensen,50,Pilot', '323,Rita,Yates,34,Librarian', '324,Annie,Haynes,51,Social worker', '325,Ruth,Harmon,75,Artist', '326,Michelle,Matthews,67,Computer software engineer', '327,Heather,Dawson,43,', '328,Jane,Barefoot,53,Accountant', '329,Wallace,Kaplan,35,Secretary', '330,Kimberly,Gross,38,Designer', '331,Jon,Richmond,37,Agricultural and food scientist', '332,Lee,Pope,55,Statistician', '333,Kenneth,Pickett,28,Police officer', '334,Keith,Schwartz,65,Childcare worker', '335,Jim,Singleton,55,Accountant', '336,Steven,Ballard,62,Pilot', '337,Pauline,Spivey,55,Computer hardware engineer', '338,Marc,Denton,71,Musician', '339,Kerry,Huff,38,Writer', '340,Lynne,Mangum,37,Chemist', '341,Claude,Berger,51,Electrical engineer', '342,Rose,McCall,29,Farmer', '343,Kathy,Pollard,43,Politician', '344,Catherine,Garcia,69,Athlete', '345,Frances,Wagner,28,Veterinarian', '346,Diana,Crane,26,Lawyer', '347,Leslie,Wolf,50,Loan officer', '348,Marian,Crane,37,Automotive mechanic', '349,Russell,Dalton,21,Civil engineer', '350,Sheryl,Diaz,30,Pharmacist', '351,Lewis,Currin,73,Farmer', '352,Ernest,Stanton,51,Lawyer', '353,Vivian,Carey,64,Statistician', '354,Harold,Li,49,Doctor', '355,Malcolm,Chan,27,Environmental scientist', '356,Kay,Hess,56,Childcare worker', '357,Max,Robinson,32,Dancer', '358,Vicki,Mills,56,Computer support specialist', '359,Leon,Bender,22,Firefighter', '360,Alvin,McDonald,73,Musician', '361,Jacob,Moore,64,Environmental scientist', '362,Holly,Fox,74,Physicist', '363,Audrey,Lanier,54,Human resources assistant', '364,Christina,Harris,39,Doctor', '365,Joseph,Underwood,57,Reporter', '366,Evelyn,Parsons,37,Human resources assistant', '367,Nicholas,Vaughn,66,Carpenter', '368,Ben,Banks,60,Accountant', '369,Dorothy,Sherrill,28,Agricultural and food scientist', '370,Theresa,Oakley,35,Nurse', '371,Harriet,Rubin,71,Real estate agent', '372,Barry,Maynard,53,Pharmacist', '373,Jerome,Hill,66,Computer hardware engineer', '374,Jason,Livingston,67,Civil engineer', '375,Norman,Lam,63,Teacher', '376,Glen,Thompson,40,Coach', '377,Ellen,Creech,36,Human resources assistant', '378,Dianne,Dillon,34,Recreation and fitness worker', '379,Harry,Foster,50,Coach', '380,Tim,Starr,59,Photographer', '381,Kent,Roy,31,Electrical engineer', '382,Eddie,Barbour,61,Athlete', '383,Arnold,Burke,70,Reporter', '384,Melanie,Ritchie,36,Financial analyst', '385,Ronald,Odom,49,Psychologist', '386,Gene,Pearce,30,Architect', '387,Peter,Rosenberg,60,Politician', '388,Jean,Garrett,73,Doctor', \"389,Neal,O'Connor,26,Therapist\", '390,Tracy,Cates,54,Dancer', '391,Holly,McIntosh,73,Computer software engineer', '392,Emma,Olson,74,Pilot', '393,Molly,Cox,55,Social worker', '394,Carlos,Erickson,63,Statistician', '395,Betsy,Chang,54,Designer', '396,Andrew,Briggs,45,Farmer', '397,Bill,Klein,26,Teacher', '398,Eileen,Goldberg,57,Therapist', '399,Randall,Hinson,51,Judge', '400,Kelly,Weiss,72,Librarian', '401,Elaine,Pritchard,48,Automotive mechanic', '402,Rhonda,Goldman,38,Computer support specialist', '403,Leo,Lassiter,65,Pilot', '404,Clarence,Massey,74,Politician', '405,Grace,Stark,41,Pharmacist', '406,Jordan,Dunlap,59,Agricultural and food scientist', '407,Glenda,Humphrey,32,Carpenter', '408,Jacob,Singleton,33,Veterinarian', '409,Jordan,Horowitz,58,Actor', '410,Dawn,Lutz,66,Therapist', '411,Joanna,Hoover,50,', '412,Sandra,Kang,47,Designer', '413,Stephen,Melton,33,Athlete', '414,Jessica,Teague,72,Firefighter', '415,Regina,Ellington,52,Social worker', '416,Paige,Cherry,45,Environmental scientist', '417,Joyce,Jennings,61,Social worker', '418,Tamara,Creech,46,Social worker', '419,Toni,Lynn,67,Civil engineer', '420,Betty,Albright,64,Engineering technician', '421,Jerry,Alston,31,Lawyer', '422,Tommy,Burnette,72,Doctor', \"423,Alan,O'Neal,59,Pilot\", '424,Christina,Morris,56,Dancer', '425,Dean,Lutz,26,Pilot', '426,Veronica,Callahan,31,Veterinarian', '427,Kelly,Conway,36,Designer', '428,Marcus,Harvey,62,Computer support specialist', '429,Carrie,Watson,52,Musician', '430,George,Glover,41,Nurse', '431,Nina,Savage,35,Politician', '432,Paige,Henson,61,Dancer', '433,Jay,Wang,64,Carpenter', '434,Sue,Ellis,59,Environmental scientist', '435,Brenda,Barbour,36,Architect', '436,Joshua,Sherrill,43,Politician', '437,Nelson,Pierce,69,Actor', '438,Vincent,Woodward,34,Electrical engineer', '439,Nicholas,Godfrey,69,Environmental scientist', '440,Hannah,Langston,25,Pilot', '441,Alice,Eaton,32,Chemist', '442,Elisabeth,Lowe,34,Farmer', '443,Matthew,Stanton,51,Secretary', '444,Thomas,Fuller,51,Childcare worker', '445,Benjamin,Simmons,70,Veterinarian', '446,Kristine,Schultz,66,Architect', '447,Jordan,Knight,65,Electrician', '448,Guy,Klein,34,Designer', '449,Terry,Garcia,30,Automotive mechanic', '450,Floyd,Schroeder,37,Farmer', '451,Russell,Hess,21,Architect', '452,Marsha,Gold,36,Athlete', '453,Kim,Hensley,64,Photographer', '454,Virginia,Turner,22,Computer hardware engineer', '455,Deborah,French,56,Accountant', '456,Peter,Hughes,38,Computer software engineer', '457,Bernard,Pate,34,Doctor', '458,Harold,Burnett,40,Musician', '459,Gretchen,Francis,60,Politician', '460,Cheryl,Horn,40,Veterinarian', '461,Audrey,Forrest,50,Architect', '462,Alan,Levin,43,Statistician', '463,Wayne,Weiner,70,', '464,Nancy,Durham,60,Nurse', '465,Peter,Guthrie,21,Nurse', '466,Benjamin,Hensley,37,Pilot', '467,Derek,Freedman,66,Recreation and fitness worker', '468,Marlene,Wiggins,65,Agricultural and food scientist', '469,Clara,Best,51,Pharmacist', '470,Ashley,Beatty,46,Computer support specialist', '471,Kara,Crawford,40,Social worker', '472,Kristin,Drake,22,Financial analyst', '473,Edwin,Curtis,25,Civil engineer', '474,Molly,Walter,30,Architect', '475,Marshall,Dunlap,43,Statistician', '476,Elaine,Jenkins,47,Writer', '477,Victor,Hood,48,Firefighter', '478,Tonya,Ellis,26,Firefighter', '479,Cecil,Jiang,74,Photographer', '480,Melissa,Johnson,64,Recreation and fitness worker', '481,Christina,Craig,48,Dancer', '482,Allison,Norman,25,Musician', '483,Gretchen,McIntyre,38,Statistician', '484,Bonnie,Brantley,68,Firefighter', '485,George,Kelley,58,Judge', '486,Barbara,Smith,58,Designer', '487,Evelyn,Lyons,51,Childcare worker', '488,Rita,Wall,58,Photographer', '489,Dan,Quinn,31,Agricultural and food scientist', '490,Juan,Hicks,37,Accountant', '491,Jesse,Garrison,59,Pharmacist', '492,Ronnie,Watts,29,Actor', '493,Hilda,Dickerson,53,Farmer', '494,Stephen,Waller,75,', '495,Lorraine,Carter,65,Financial analyst', '496,Chris,Robinson,74,Economist', '497,Katie,Katz,26,Dancer', '498,Richard,Hull,69,Accountant', '499,Steve,Bowling,61,Recreation and fitness worker', '500,Monica,Brantley,58,Engineering technician', '501,Kenneth,Brock,68,Loan officer', '502,Leigh,James,37,Teacher', '503,Glenda,McMillan,61,Police officer', '504,Ricky,Hu,50,Teacher', '505,Rick,Waller,54,Teacher', '506,Wanda,Abbott,36,Agricultural and food scientist', '507,Heidi,McKee,68,Engineering technician', '508,Robin,Waters,63,Computer hardware engineer', '509,Dan,Sims,33,Loan officer', '510,Kathy,Henderson,69,Coach', '511,Alicia,Rao,68,Loan officer', '512,Sherry,Bray,67,Chemist', '513,Chris,Scarborough,45,Engineering technician', '514,Nathan,Ford,36,Financial analyst', '515,Sidney,Blum,54,Photographer', '516,Clyde,Kenney,25,Pharmacist', '517,Barry,Gordon,58,Actor', '518,Shannon,Blair,72,Librarian', '519,Melinda,Moore,67,Chemist', '520,Maureen,Kemp,45,Artist', '521,Pat,Hutchinson,42,Computer support specialist', '522,Neal,Brennan,28,Therapist', '523,Anne,Little,64,Nurse', '524,Clifford,Gill,40,Computer support specialist', '525,Nicole,Keller,55,Human resources assistant', '526,Rachel,Rosenthal,56,Automotive mechanic', '527,Carolyn,McConnell,31,Firefighter', '528,Samuel,Sawyer,63,Pharmacist', '529,Theodore,McCall,68,Lawyer', '530,Eleanor,Coates,31,Reporter', '531,Glenn,Hicks,40,Librarian', '532,Wesley,Davidson,73,Electrical engineer', '533,Ralph,Hawkins,34,Agricultural and food scientist', '534,Annie,Lindsay,38,Statistician', '535,Beth,Gonzalez,41,Human resources assistant', '536,Claire,Gray,66,Therapist', '537,Jonathan,English,67,Agricultural and food scientist', '538,Bradley,Duke,27,Police officer', '539,Hugh,Webb,71,Firefighter', '540,Nicholas,Baldwin,71,Childcare worker', '541,Penny,Lamb,32,Agricultural and food scientist', '542,Gloria,Shaffer,33,Pilot', '543,Kelly,Wang,31,Accountant', '544,Tommy,Burgess,38,Librarian', '545,Danielle,Smith,22,Pharmacist', '546,Calvin,Fletcher,55,Judge', '547,Emily,Boyd,61,Psychologist', '548,Luis,Hirsch,72,Librarian', '549,Kimberly,Currie,74,Financial analyst', '550,Paige,McKenzie,36,Chemist', '551,Sidney,Weber,26,Electrician', '552,Robert,Honeycutt,63,Computer hardware engineer', '553,Karen,Manning,30,Recreation and fitness worker', '554,Kyle,Bolton,25,Writer', '555,Mary,Ritchie,57,Politician', '556,Evelyn,Baldwin,71,Statistician', '557,Lynda,Riley,38,Electrician', '558,Beth,Swanson,64,Photographer', '559,Erin,Huffman,37,Electrician', '560,James,Gibson,35,Computer support specialist', '561,Leigh,Yates,45,Secretary', '562,Jordan,Wrenn,61,Pilot', '563,Willie,Green,34,Civil engineer', '564,Wade,Harris,63,Pharmacist', '565,Lori,Hayes,51,Firefighter', '566,Charlotte,Hamrick,31,Therapist', '567,Rhonda,Hawley,37,Reporter', '568,Steve,Koch,36,Nurse', '569,Nicholas,McKenzie,33,Actor', '570,Vicki,Harrell,74,Civil engineer', '571,Stephanie,Parsons,29,Politician', '572,Jose,McGuire,61,Photographer', '573,George,Stephenson,42,Electrician', '574,Marlene,Baxter,37,Recreation and fitness worker', '575,Catherine,Summers,27,Real estate agent', '576,Carrie,Welch,67,Doctor', '577,Leo,Nixon,46,Coach', '578,Rita,Kelly,32,Automotive mechanic', '579,Joan,Sumner,28,Accountant', '580,Marsha,Cobb,56,Chemist', '581,Wesley,Bruce,39,Civil engineer', '582,Cathy,Newton,37,Teacher', '583,Kevin,Rogers,66,Lawyer', '584,Craig,Sanchez,60,Chemist', '585,Lester,Finch,65,Human resources assistant', '586,Joy,Silverman,28,Coach', '587,Kate,Horn,54,Doctor', '588,Bernice,Richardson,71,Real estate agent', '589,Theodore,Gay,60,Accountant', '590,Cheryl,Chase,53,Secretary', '591,Jamie,Gallagher,69,Librarian', '592,Audrey,Kern,50,Nurse', '593,Jennifer,Scott,43,Carpenter', '594,Lewis,Bradley,31,Carpenter', '595,Ben,Puckett,25,Photographer', '596,Allen,Sanchez,59,Recreation and fitness worker', '597,Alex,Yang,62,Athlete', '598,Jennifer,Brantley,47,Musician', '599,Doris,Bunn,43,Veterinarian', '600,Jonathan,Link,44,Financial analyst', '601,Allan,Nguyen,51,Pilot', '602,Shawn,Stephens,32,Veterinarian', '603,Stephanie,Horne,60,Real estate agent', '604,Carol,Burton,39,Doctor', '605,Melanie,Diaz,56,Musician', '606,Samantha,Berry,31,Pilot', '607,Renee,Knowles,63,Judge', '608,Cathy,Freeman,42,Computer hardware engineer', '609,Heidi,Hernandez,71,Nurse', '610,Gayle,Roach,50,Athlete', '611,Rita,Hardison,29,Athlete', '612,Jacob,Wolf,54,Statistician', '613,Steven,Boyd,71,Physicist', '614,Tracy,Caldwell,61,Electrician', '615,Eva,Mann,30,Police officer', '616,Lorraine,McLeod,43,Physicist', '617,Willie,Stanton,63,Musician', '618,Joel,Park,33,Engineering technician', '619,Vanessa,Chang,63,Psychologist', '620,Becky,Newton,42,Judge', '621,Melinda,Phillips,28,Civil engineer', '622,Harvey,Whitaker,44,Farmer', '623,Lee,Pitts,40,Photographer', '624,Aaron,McLean,35,Social worker', '625,Jack,Barton,30,Veterinarian', '626,Suzanne,Gould,67,Loan officer', '627,Katherine,Atkins,69,Recreation and fitness worker', '628,Robyn,Shapiro,58,Architect', '629,Julia,Vincent,49,Therapist', '630,Bob,Harrell,40,Electrician', '631,Brooke,Boswell,66,Childcare worker', '632,Janet,Lassiter,58,Financial analyst', '633,Kristen,Fisher,59,Recreation and fitness worker', '634,Jenny,Case,64,Agricultural and food scientist', '635,Jeanette,Parsons,63,Architect', '636,Megan,McPherson,46,Athlete', '637,Joel,Wiley,42,Architect', '638,Don,Schwartz,35,Coach', '639,Florence,McFarland,23,Electrical engineer', '640,Jon,Baker,53,Civil engineer', '641,Annie,Holden,36,Childcare worker', '642,Kent,Hartman,67,Photographer', '643,Eleanor,Schwartz,56,Computer support specialist', '644,Nina,Nguyen,62,Doctor', '645,Clara,Houston,53,Statistician', '646,Bonnie,Friedman,57,Computer hardware engineer', '647,Dwight,Adcock,67,Recreation and fitness worker', '648,Angela,Stephens,53,Secretary', '649,Marion,McClure,35,Coach', '650,Curtis,Proctor,63,Computer hardware engineer', '651,Tammy,Lang,63,Musician', '652,Gilbert,Berger,32,Athlete', '653,Edwin,Aldridge,28,Pilot', '654,Emily,Davies,34,Coach', '655,Hannah,Wall,23,Librarian', '656,Ronald,Miles,64,Dancer', '657,Helen,Bolton,62,Librarian', '658,Barbara,Morgan,53,Childcare worker', '659,Milton,Fisher,29,Dancer', '660,Anne,Stephens,63,Carpenter', '661,Lois,Holmes,31,Photographer', '662,Dorothy,Ferrell,53,Firefighter', '663,Claire,Henry,29,Police officer', '664,Anita,Hedrick,63,Computer software engineer', '665,Ronald,Horne,74,Architect', '666,Allen,Weiss,38,Computer hardware engineer', '667,Zachary,Singh,66,Firefighter', '668,Kyle,Blalock,70,Politician', '669,Seth,Aldridge,51,Judge', '670,Jeff,Ritchie,65,Photographer', '671,Sandra,Grossman,50,Coach', '672,Jackie,Pugh,39,Real estate agent', '673,Audrey,Olson,36,Computer support specialist', '674,Dana,Fernandez,39,Recreation and fitness worker', '675,Hazel,Arnold,69,Veterinarian', '676,Joann,Stanley,47,Computer hardware engineer', '677,Audrey,Field,70,Carpenter', '678,Walter,Farmer,50,Human resources assistant', '679,Herbert,Jernigan,59,', 'HI,Hawaii', 'ID,Idaho', 'IL,Illinois', 'IN,Indiana', 'IA,Iowa', 'KS,Kansas', 'KY,Kentucky', 'LA,Louisiana', 'ME,Maine', 'MT,Montana', 'NE,Nebraska', 'NV,Nevada', 'NH,New Hampshire', 'NJ,New Jersey', '680,Harold,Bowers,57,Librarian', '681,Sherry,Crabtree,55,Actor', '682,Jeanne,Crabtree,24,Pharmacist', '683,Rick,Clements,61,Nurse', '684,Kristen,Spivey,59,Chemist', '685,Patrick,Archer,27,Veterinarian', '686,Sherri,Owen,66,Childcare worker', '687,Herbert,Strickland,74,Accountant', '688,Miriam,Berg,25,Judge', '689,Maurice,Gibbons,60,Coach', '690,Sandy,Warner,55,Judge', '691,Shannon,Bray,69,Designer', '692,Bradley,Eason,39,Doctor', '693,Lester,Hoover,67,Computer hardware engineer', '694,Tom,Park,54,Doctor', '695,Chris,Anderson,52,', '696,Milton,Li,33,Doctor', '697,Eugene,Elmore,31,Dancer', '698,Ashley,Pearson,52,Judge', '699,Lois,Harper,36,Politician', '700,Annette,Chu,39,Computer support specialist', '701,Jenny,Schultz,56,Computer support specialist', '702,Zachary,Black,26,Childcare worker', '703,Barry,Mitchell,66,Pharmacist', '704,Jerome,Sharp,56,Athlete', '705,Cynthia,Glover,69,Physicist', '706,Marilyn,Cates,32,Automotive mechanic', '707,Natalie,Martin,52,Physicist', '708,Brenda,Lowry,57,Statistician', '709,Marsha,Cooke,28,Photographer', '710,William,Fink,44,Veterinarian', '711,Gloria,Barrett,36,Farmer', '712,Lester,Olson,62,Lawyer', '713,Vanessa,Melton,40,Veterinarian', '714,Pauline,Coley,74,Farmer', '715,Hannah,Mueller,26,Artist', '716,Warren,Paul,32,Politician', '717,Teresa,Daniel,26,Statistician', '718,Joan,Padgett,39,Agricultural and food scientist', '719,Jonathan,Daniels,28,Writer', '720,Denise,Hayes,34,Psychologist', '721,Bobby,Hines,26,Pilot', '722,Erin,Pridgen,44,Carpenter', '723,Harry,Stone,64,Computer support specialist', '724,Priscilla,Hayes,28,Computer support specialist', '725,Renee,Harris,65,Architect', '726,Bob,Walter,72,Statistician', '727,Julian,Woods,40,Financial analyst', '728,Annie,Jennings,35,Nurse', '729,Wendy,Lopez,37,Politician', '730,Bernard,McCarthy,41,Electrician', '731,Alvin,Frederick,27,Recreation and fitness worker', '732,Denise,Lopez,60,Veterinarian', '733,Gary,Scarborough,27,Firefighter', '734,Oscar,Brandt,65,Firefighter', '735,Nina,Nolan,70,Librarian', '736,Ann,Chandler,25,Electrician', '737,Shirley,Carlton,57,Librarian', '738,Joanne,Katz,60,Recreation and fitness worker', '739,Patsy,Parrott,41,Therapist', '740,Franklin,Corbett,75,Firefighter', '741,Bradley,Godfrey,34,Photographer', '742,Marguerite,Cooke,56,Computer support specialist', '743,Sherri,Pate,61,Accountant', '744,Dorothy,Barber,39,Firefighter', '745,Brent,Fletcher,68,Photographer', '746,Regina,Schroeder,34,Environmental scientist', '747,Calvin,Lindsay,35,Firefighter', '748,Gilbert,Boswell,37,Financial analyst', '749,Wesley,Buckley,60,Nurse', '750,Erika,Harmon,53,Dancer', '751,Don,Walters,70,Electrical engineer', '752,Ethel,Stevens,23,Designer', '753,Rodney,Knight,29,Carpenter', '754,Dennis,Rowland,72,Coach', '755,Zachary,Lindsay,67,Agricultural and food scientist', '756,Harriet,Bowling,40,Pharmacist', '757,Amanda,Kirby,24,Electrician', '758,Jose,Benson,65,Computer hardware engineer', '759,Lynn,Anthony,43,Social worker', '760,Shelley,Dunn,27,Chemist', '761,Mike,Hill,51,Chemist', '762,Laurence,Lang,58,Teacher', '763,Amanda,Grimes,31,Physicist', '764,Bernard,Bowers,38,Loan officer', '765,Janice,Bowden,21,Politician', '766,Hazel,Underwood,71,Agricultural and food scientist', '767,Luis,Zhang,24,Pharmacist', '768,Ralph,Godwin,54,Judge', '769,Geraldine,Rice,53,Judge', '770,Claire,Townsend,57,Financial analyst', '771,Jeanne,Lin,70,Computer hardware engineer', '772,Willie,Pitts,29,Actor', '773,Alfred,Koch,62,Judge', '774,Eddie,Callahan,43,Pilot', '775,Christopher,Long,34,Environmental scientist', '776,Phyllis,Norton,28,Economist', '777,Peggy,Blackburn,57,Police officer', \"778,Elaine,O'Connell,70,Agricultural and food scientist\", '779,Jason,Bowling,21,Coach', '780,Neal,Robinson,75,Artist', '781,Diana,Pritchard,73,Therapist', '782,Nina,Lawson,51,Loan officer', '783,Maxine,Dickerson,57,Photographer', '784,Ted,Livingston,70,Economist', 'AL,Alabama', 'AK,Alaska', 'AZ,Arizona', 'AR,Arkansas', 'CA,California', 'CO,Colorado', 'CT,Connecticut', 'DE,Delaware', 'DC,District of Columbia', 'FL,Florida', 'GA,Georgia', '785,Allen,Hansen,33,Secretary', '786,Gordon,Berman,41,Teacher', '787,Billy,Carroll,27,Librarian', '788,Veronica,Kearney,38,Photographer', '789,Hazel,Peterson,60,Designer', '790,Eddie,Richards,40,Chemist', '791,Katherine,Sutherland,25,Dancer', '792,William,McCormick,64,Politician', '793,Nina,Beach,48,Civil engineer', '794,Ben,Wu,51,Environmental scientist', '795,Frances,Hunt,33,Artist', '796,Thelma,Carver,71,Teacher', '797,Dennis,Anthony,56,Musician', '798,Christina,Livingston,48,Police officer', '799,James,Floyd,51,Accountant', '800,Frederick,McCall,67,Veterinarian', '801,Alfred,Haynes,36,Farmer', '802,Oscar,Gunter,36,Musician', '803,Joanne,Solomon,48,Doctor', '804,Richard,Harris,27,Actor', '805,Christine,Cline,36,Police officer', '806,Theodore,McKay,33,Accountant', '807,Jack,Braun,68,Financial analyst', '808,Rhonda,Preston,49,Actor', '809,Beth,Hayes,59,Designer', '810,Douglas,Burnette,41,Secretary']\n12.Split the above data into 2 RDDs\n12.custfilter with 5 columns data\n12.statesfilter with 2 columns data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#11\n",
    "rdd1=sc.textFile(path_custs_states)\n",
    "print(\"11.Load the file3 (custs_states.csv) from the DBFS location using RDD\")\n",
    "print(rdd1.collect())\n",
    "#12\n",
    "split_rdd = rdd1.map(lambda line: line.split(\",\"))\n",
    "custfilter = split_rdd.filter(lambda x: len(x) == 5)\n",
    "statesfilter = split_rdd.filter(lambda x: len(x) == 2)\n",
    "print(\"12.Split the above data into 2 RDDs\")\n",
    "print(\"12.custfilter with 5 columns data\")\n",
    "#print(custfilter.collect())\n",
    "print(\"12.statesfilter with 2 columns data\")\n",
    "#print(statesfilter.collect())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12c37d50-2253-4990-9aa9-bda4e5c1f78e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "****Use DSL Functions****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe25ba7-5895-4495-9f71-382febdbbb7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.Load the file3 (custs_states.csv) from the HDFS location using DSL\n+---+--------------+-----+----+-------+\n|_c0|           _c1|  _c2| _c3|    _c4|\n+---+--------------+-----+----+-------+\n|  1|      Kristina|Chung|  55|  Pilot|\n|  2|         Paige| Chen|  74|Teacher|\n| NM|    New Mexico| null|null|   null|\n| NY|      New York| null|null|   null|\n| NC|North Carolina| null|null|   null|\n| ND|  North Dakota| null|null|   null|\n| OH|          Ohio| null|null|   null|\n| OK|      Oklahoma| null|null|   null|\n| OR|        Oregon| null|null|   null|\n| MD|      Maryland| null|null|   null|\n| MA| Massachusetts| null|null|   null|\n| MI|      Michigan| null|null|   null|\n| MN|     Minnesota| null|null|   null|\n| MS|   Mississippi| null|null|   null|\n| MO|      Missouri| null|null|   null|\n| PA|  Pennsylvania| null|null|   null|\n| RI|  Rhode Island| null|null|   null|\n| SC|South Carolina| null|null|   null|\n| SD|  South Dakota| null|null|   null|\n| TN|     Tennessee| null|null|   null|\n+---+--------------+-----+----+-------+\nonly showing top 20 rows\n\n13.Split the above data into 2 DFs\nCustomer Master Info (5 columns):\n+------+---------+----------+---+---------------------------+\n|custid|firstname|lastname  |age|profession                 |\n+------+---------+----------+---+---------------------------+\n|1     |Kristina |Chung     |55 |Pilot                      |\n|2     |Paige    |Chen      |74 |Teacher                    |\n|3     |Sherri   |Melton    |34 |Firefighter                |\n|4     |Gretchen |Hill      |66 |Computer hardware engineer |\n|5     |Karen    |Puckett   |74 |Lawyer                     |\n|6     |Patrick  |Song      |42 |Veterinarian               |\n|7     |Elsie    |Hamilton  |43 |Pilot                      |\n|8     |Hazel    |Bender    |63 |Carpenter                  |\n|9     |Malcolm  |Wagner    |39 |Artist                     |\n|10    |Dolores  |McLaughlin|60 |Writer                     |\n|11    |Francis  |McNamara  |47 |Therapist                  |\n|12    |Sandy    |Raynor    |26 |Writer                     |\n|13    |Marion   |Moon      |41 |Carpenter                  |\n|15    |Julia    |Desai     |49 |Musician                   |\n|16    |Jerome   |Wallace   |52 |Pharmacist                 |\n|17    |Neal     |Lawrence  |72 |Computer support specialist|\n|18    |Jean     |Griffin   |45 |Childcare worker           |\n|19    |Kristine |Dougherty |63 |Financial analyst          |\n|20    |Crystal  |Powers    |67 |Engineering technician     |\n|21    |Alex     |May       |39 |Environmental scientist    |\n+------+---------+----------+---+---------------------------+\nonly showing top 20 rows\n\nState Description (2 columns):\n+------+-----------------+\n|stated|state_description|\n+------+-----------------+\n|NM    |New Mexico       |\n|NY    |New York         |\n|NC    |North Carolina   |\n|ND    |North Dakota     |\n|OH    |Ohio             |\n|OK    |Oklahoma         |\n|OR    |Oregon           |\n|MD    |Maryland         |\n|MA    |Massachusetts    |\n|MI    |Michigan         |\n|MN    |Minnesota        |\n|MS    |Mississippi      |\n|MO    |Missouri         |\n|PA    |Pennsylvania     |\n|RI    |Rhode Island     |\n|SC    |South Carolina   |\n|SD    |South Dakota     |\n|TN    |Tennessee        |\n|TX    |Texas            |\n|UT    |Utah             |\n+------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#13\n",
    "custstatesdf = spark.read.csv(path_custs_states, header=False, inferSchema=True)\n",
    "print(\"13.Load the file3 (custs_states.csv) from the DBFS location using DSL\")\n",
    "custstatesdf.show()\n",
    "#14\n",
    "custfilterdf = custstatesdf.filter(\n",
    "    custstatesdf[\"_c2\"].isNotNull() & custstatesdf[\"_c3\"].isNotNull() & custstatesdf[\"_c4\"].isNotNull()\n",
    ").selectExpr(\n",
    "    \"_c0 as custid\", \"_c1 as firstname\", \"_c2 as lastname\", \"_c3 as age\", \"_c4 as profession\"\n",
    ")\n",
    "\n",
    "statesfilterdf = custstatesdf.filter(\n",
    "    custstatesdf[\"_c2\"].isNull() & custstatesdf[\"_c3\"].isNull() & custstatesdf[\"_c4\"].isNull()\n",
    ").selectExpr(\n",
    "    \"_c0 as stated\", \"_c1 as state_description\"\n",
    ")\n",
    "print(\"13.Split the above data into 2 DFs\")\n",
    "print(\"Customer Master Info (5 columns):\")\n",
    "custfilterdf.show(truncate=False)\n",
    "print(\"State Description (2 columns):\")\n",
    "statesfilterdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e99de6f-71dd-4a65-8b18-512cf64a1e77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "****Use SQL Queries****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "472cef7a-5b9f-40ba-a898-910cd7605ada",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.custfilterdf stored in temp view custview\n15.statesfilterdf stored in temp view statesview\n16.merged_df stored in temp view insureview\n17. spark.udf.register('remspecialchar_udf', remspecialchar_udf) \n18.a Pass NetworkName to remspecialcharudf and get the new column called cleannetworkname\n18.b Add current date, current timestamp fields as curdt and curts\n18.c year and month from the businessdate into two columns yr,mth \n18.d NetworkURl secured/non secured/no protocol\n+--------------------+--------------------+----------+--------------------+----+---+----------------+\n|         NetworkName|    cleannetworkname|     curdt|               curts|  yr|mth|        protocol|\n+--------------------+--------------------+----------+--------------------+----+---+----------------+\n|EHB Basic Dental ...|EHB Basic Dental ...|2024-07-21|2024-07-21 15:28:...|2019| 10|   https secured|\n|EHB Enhanced Dent...|EHB Enhanced Dent...|2024-07-21|2024-07-21 15:28:...|2019| 10|   https secured|\n|Family Basic Dent...|Family Basic Dent...|2024-07-21|2024-07-21 15:28:...|2019| 10|   https secured|\n|Family Enhanced D...|Family Enhanced D...|2024-07-21|2024-07-21 15:28:...|2019| 10|   https secured|\n|Total Dental Admi...|Total Dental Admi...|2024-07-21|2024-07-21 15:28:...|2019| 10|http non secured|\n|EHB Basic Dental ...|EHB Basic Dental ...|2024-07-21|2024-07-21 15:28:...|2019| 10|   https secured|\n|EHB Enhanced Dent...|EHB Enhanced Dent...|2024-07-21|2024-07-21 15:28:...|2019| 10|   https secured|\n|Family Basic Dent...|Family Basic Dent...|2024-07-21|2024-07-21 15:28:...|2019| 10|   https secured|\n|Family Enhanced D...|Family Enhanced D...|2024-07-21|2024-07-21 15:28:...|2019| 10|   https secured|\n|EHB Basic Dental ...|EHB Basic Dental ...|2024-07-21|2024-07-21 15:28:...|2019| 10|   https secured|\n+--------------------+--------------------+----------+--------------------+----+---+----------------+\nonly showing top 10 rows\n\n18.e Join insureview,custview,stateview and output required columns\n+--------+---------+------------+----+-----+--------------------+--------------------+-------+--------------+-----------------+----------+--------------------+--------------------+----------+--------------------+----+---+-------------+---+----------+-----------------+\n|IssuerId|IssuerId2|BusinessDate|stcd|srcnm|         NetworkName|          NetworkURL|custnum|MarketCoverage|issueridcomposite|     sysdt|               systs|    cleannetworkname|     curdt|               curts|  yr|mth|     protocol|age|profession|state_description|\n+--------+---------+------------+----+-----+--------------------+--------------------+-------+--------------+-----------------+----------+--------------------+--------------------+----------+--------------------+----+---+-------------+---+----------+-----------------+\n|   17100|    17100|  2019-10-01|  AZ| HIOS|EHB Basic Dental ...|https://metlocato...|     13|          null|       1710017100|2024-07-21|2024-07-21 15:28:...|EHB Basic Dental ...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 41| Carpenter|          Arizona|\n|   17100|    17100|  2019-10-01|  AZ| HIOS|Family Basic Dent...|https://metlocato...|     15|          null|       1710017100|2024-07-21|2024-07-21 15:28:...|Family Basic Dent...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 49|  Musician|          Arizona|\n|   17100|    17100|  2019-10-01|  AZ| HIOS|Family Enhanced D...|https://metlocato...|     16|          null|       1710017100|2024-07-21|2024-07-21 15:28:...|Family Enhanced D...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 52|Pharmacist|          Arizona|\n|   30219|    30219|  2019-10-01|  FL| HIOS|EHB Basic Dental ...|https://metlocato...|     13|          null|       3021930219|2024-07-21|2024-07-21 15:28:...|EHB Basic Dental ...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 41| Carpenter|          Florida|\n|   30219|    30219|  2019-10-01|  FL| HIOS|Family Basic Dent...|https://metlocato...|     15|          null|       3021930219|2024-07-21|2024-07-21 15:28:...|Family Basic Dent...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 49|  Musician|          Florida|\n|   30219|    30219|  2019-10-01|  FL| HIOS|Family Enhanced D...|https://metlocato...|     16|          null|       3021930219|2024-07-21|2024-07-21 15:28:...|Family Enhanced D...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 52|Pharmacist|          Florida|\n|   98534|    98534|  2019-10-01|  FL| HIOS|EHB Basic Dental ...|https://metlocato...|     13|          null|       9853498534|2024-07-21|2024-07-21 15:28:...|EHB Basic Dental ...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 41| Carpenter|          Florida|\n|   98534|    98534|  2019-10-01|  FL| HIOS|Family Basic Dent...|https://metlocato...|     15|          null|       9853498534|2024-07-21|2024-07-21 15:28:...|Family Basic Dent...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 49|  Musician|          Florida|\n|   98534|    98534|  2019-10-01|  FL| HIOS|Family Enhanced D...|https://metlocato...|     16|          null|       9853498534|2024-07-21|2024-07-21 15:28:...|Family Enhanced D...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 52|Pharmacist|          Florida|\n|   55612|    55612|  2019-10-01|  GA| HIOS|EHB Basic Dental ...|https://metlocato...|     13|          null|       5561255612|2024-07-21|2024-07-21 15:28:...|EHB Basic Dental ...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 41| Carpenter|          Georgia|\n|   55612|    55612|  2019-10-01|  GA| HIOS|Family Basic Dent...|https://metlocato...|     15|          null|       5561255612|2024-07-21|2024-07-21 15:28:...|Family Basic Dent...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 49|  Musician|          Georgia|\n|   55612|    55612|  2019-10-01|  GA| HIOS|Family Enhanced D...|https://metlocato...|     16|          null|       5561255612|2024-07-21|2024-07-21 15:28:...|Family Enhanced D...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 52|Pharmacist|          Georgia|\n|   83350|    83350|  2019-10-01|  IL| HIOS|EHB Basic Dental ...|https://metlocato...|     13|          null|       8335083350|2024-07-21|2024-07-21 15:28:...|EHB Basic Dental ...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 41| Carpenter|         Illinois|\n|   83350|    83350|  2019-10-01|  IL| HIOS|Family Basic Dent...|https://metlocato...|     15|          null|       8335083350|2024-07-21|2024-07-21 15:28:...|Family Basic Dent...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 49|  Musician|         Illinois|\n|   83350|    83350|  2019-10-01|  IL| HIOS|Family Enhanced D...|https://metlocato...|     16|          null|       8335083350|2024-07-21|2024-07-21 15:28:...|Family Enhanced D...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 52|Pharmacist|         Illinois|\n|   69051|    69051|  2019-10-01|  IN| HIOS|EHB Basic Dental ...|https://www.metli...|     13|          null|       6905169051|2024-07-21|2024-07-21 15:28:...|EHB Basic Dental ...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 41| Carpenter|          Indiana|\n|   69051|    69051|  2019-10-01|  IN| HIOS|Family Basic Dent...|https://www.metli...|     15|          null|       6905169051|2024-07-21|2024-07-21 15:28:...|Family Basic Dent...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 49|  Musician|          Indiana|\n|   69051|    69051|  2019-10-01|  IN| HIOS|Family Enhanced D...|https://www.metli...|     16|          null|       6905169051|2024-07-21|2024-07-21 15:28:...|Family Enhanced D...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 52|Pharmacist|          Indiana|\n|   75409|    75409|  2019-10-01|  LA| HIOS|EHB Basic Dental ...|https://www.metli...|     13|          null|       7540975409|2024-07-21|2024-07-21 15:28:...|EHB Basic Dental ...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 41| Carpenter|        Louisiana|\n|   75409|    75409|  2019-10-01|  LA| HIOS|Family Basic Dent...|https://www.metli...|     15|          null|       7540975409|2024-07-21|2024-07-21 15:28:...|Family Basic Dent...|2024-07-21|2024-07-21 15:28:...|2019| 10|https secured| 49|  Musician|        Louisiana|\n+--------+---------+------------+----+-----+--------------------+--------------------+-------+--------------+-----------------+----------+--------------------+--------------------+----------+--------------------+----+---+-------------+---+----------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "\n",
    "custfilterdf.createOrReplaceTempView(\"custview\")\n",
    "print(\"15.custfilterdf stored in temp view custview\")\n",
    "statesfilterdf.createOrReplaceTempView(\"statesview\")\n",
    "print(\"15.statesfilterdf stored in temp view statesview\")\n",
    "#16\n",
    "merged_df.createOrReplaceTempView(\"insureview\")\n",
    "print(\"16.merged_df stored in temp view insureview\")\n",
    "#17\n",
    "spark.udf.register(\"remspecialchar_udf\", remspecialchar_udf)\n",
    "print(\"17. spark.udf.register('remspecialchar_udf', remspecialchar_udf) \")\n",
    "\n",
    "#18\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "\n",
    "#a.\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    NetworkName, \n",
    "    remspecialchar_udf(NetworkName) AS cleannetworkname,\n",
    "    current_date() AS curdt,\n",
    "    current_timestamp() AS curts,\n",
    "    year(businessdate) AS yr,\n",
    "    month(businessdate) AS mth,\n",
    "    case when NetworkURL like 'http://%' then \"http non secured\"\n",
    "    when NetworkURL like 'https://%' then \"https secured\"\n",
    "    else \"noprotocol\"\n",
    "    end as protocol\n",
    "FROM  insureview where NetworkName like '%(%' \n",
    "\"\"\"\n",
    "\n",
    "# Run the SQL query\n",
    "insureview_df = spark.sql(query)\n",
    "print(\"18.a Pass NetworkName to remspecialcharudf and get the new column called cleannetworkname\")\n",
    "print(\"18.b Add current date, current timestamp fields as curdt and curts\")\n",
    "print(\"18.c year and month from the businessdate into two columns yr,mth \")\n",
    "print(\"18.d NetworkURl secured/non secured/no protocol\")\n",
    "insureview_df.show(10)\n",
    "\n",
    "print(\"18.e Join insureview,custview,stateview and output required columns\")\n",
    "join_query = \"\"\"\n",
    "SELECT \n",
    "    a.*, \n",
    "    remspecialchar_udf(NetworkName) AS cleannetworkname,\n",
    "    current_date() AS curdt,\n",
    "    current_timestamp() AS curts,\n",
    "    year(businessdate) AS yr,\n",
    "    month(businessdate) AS mth,\n",
    "    case when NetworkURL like 'http://%' then \"http non secured\"\n",
    "    when NetworkURL like 'https://%' then \"https secured\"\n",
    "    else \"noprotocol\"\n",
    "    end as protocol,\n",
    "    age,profession,state_description\n",
    "FROM  insureview a inner join statesview b on a.stcd=b.stated inner join custview c on c.custid=a.custnum where NetworkName like '%(%' \n",
    "\"\"\"\n",
    "\n",
    "insureview_cust_state_join_df=spark.sql(join_query)\n",
    "insureview_cust_state_join_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb7b2e87-4988-47b8-91e5-b2ce6413699b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "****Store Dataframe in Parquet formats****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e4baddc-1939-47fb-bc03-be002c67656f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19. stored above join df in a single parquet file in DBFS in dbfs:/FileStore/hackathon//insureview_cust_state_join_df\n"
     ]
    }
   ],
   "source": [
    "#19\n",
    "print(\"19. stored above join df in a single parquet file in DBFS in dbfs:/FileStore/hackathon//insureview_cust_state_join_df\")\n",
    "insureview_cust_state_join_df.coalesce(1).write.mode(\"overwrite\").parquet(\"dbfs:/FileStore/hackathon//insureview_cust_state_join_df\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c764ac51-6d32-413f-b37b-4f0f2141f8e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "****Use Window Function for Data Analysis****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "508ec2bc-1f7d-4f23-916b-02b7f8a2c62d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.Display second highest profession counts done in each state and partitioned by protocol\n+-----+--------------------+----------------+-----------+-----------------+----------------+\n|seqno|          profession|profession_count|average_age|state_description|        protocol|\n+-----+--------------------+----------------+-----------+-----------------+----------------+\n|    1|           Carpenter|               1|       41.0|           Kansas|http non secured|\n|    2|Environmental sci...|               1|       39.0|         Oklahoma|http non secured|\n|    1|            Musician|               3|       49.0|          Florida|   https secured|\n|    2|            Musician|               2|       49.0|          Arizona|   https secured|\n|    3|          Pharmacist|               2|       52.0|          Indiana|   https secured|\n|    4|          Pharmacist|               2|       52.0|         Missouri|   https secured|\n|    5|          Pharmacist|               2|       52.0|       New Jersey|   https secured|\n|    6|          Pharmacist|               1|       52.0|          Georgia|   https secured|\n|    7|            Musician|               1|       49.0|         Illinois|   https secured|\n|    8|            Musician|               1|       49.0|        Louisiana|   https secured|\n+-----+--------------------+----------------+-----------+-----------------+----------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#20\n",
    "print(\"20.Display second highest profession counts done in each state and partitioned by protocol\")\n",
    "insureview_cust_state_join_df.createOrReplaceTempView(\"insureview_cust_state_join_df\")\n",
    "# Combined SQL query\n",
    "cte_query = \"\"\"\n",
    "WITH aggregated AS (\n",
    "    SELECT\n",
    "        state_description,\n",
    "        protocol,\n",
    "        profession,\n",
    "        AVG(age) AS avgage,\n",
    "        COUNT(*) AS profession_count\n",
    "    FROM\n",
    "        insureview_cust_state_join_df\n",
    "    GROUP BY\n",
    "        state_description,\n",
    "        protocol,\n",
    "        profession\n",
    "),\n",
    "ranked AS (\n",
    "    SELECT\n",
    "        state_description,\n",
    "        protocol,\n",
    "        profession,\n",
    "        avgage,\n",
    "        profession_count,\n",
    "        ROW_NUMBER() OVER (PARTITION BY state_description, protocol ORDER BY profession_count DESC) AS rank\n",
    "    FROM\n",
    "        aggregated\n",
    ")\n",
    "SELECT\n",
    "    ROW_NUMBER() OVER (PARTITION BY protocol ORDER BY profession_count DESC) AS seqno,\n",
    "    profession,\n",
    "    profession_count,\n",
    "    avgage as average_age,\n",
    "    state_description,\n",
    "    protocol\n",
    "    \n",
    "FROM\n",
    "    ranked\n",
    "WHERE\n",
    "    rank = 2   \n",
    "\"\"\"\n",
    "\n",
    "# Execute the combined SQL query\n",
    "profession_count_df = spark.sql(cte_query)\n",
    "\n",
    "# Show the result\n",
    "profession_count_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1438177e-9437-44da-ad88-d53b21f08376",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "****Write Dataframe to Sql DB Table****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea4bd758-cfd6-40d1-ad69-76e3956c317a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-375575487821409>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#21\u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m \u001B[43mprofession_count_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjdbc\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mjdbc:mysql://127.0.0.1/hackathon\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mtable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mT_profession_count\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mproperties\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mroot\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpassword\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mRoot123$\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdriver\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcom.mysql.cj.jdbc.Driver\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mqueryTimeout\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m30\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtruncate\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrue\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;124;03mCREATE TABLE T_profession_count (\u001B[39;00m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;124;03m  seqno int DEFAULT NULL,\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     11\u001B[0m \u001B[38;5;124;03m  state_description varchar(255) DEFAULT NULL\u001B[39;00m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;124;03m)'''\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1915\u001B[0m, in \u001B[0;36mDataFrameWriter.jdbc\u001B[0;34m(self, url, table, mode, properties)\u001B[0m\n",
       "\u001B[1;32m   1913\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m properties:\n",
       "\u001B[1;32m   1914\u001B[0m     jprop\u001B[38;5;241m.\u001B[39msetProperty(k, properties[k])\n",
       "\u001B[0;32m-> 1915\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjdbc\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjprop\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o1216.jdbc.\n",
       ": com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure\n",
       "\n",
       "The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.\n",
       "\tat com.mysql.cj.jdbc.exceptions.SQLError.createCommunicationsException(SQLError.java:174)\n",
       "\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:64)\n",
       "\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:836)\n",
       "\tat com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:456)\n",
       "\tat com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:246)\n",
       "\tat com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:198)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n",
       "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:123)\n",
       "\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:119)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.saveTableToJDBC(JdbcRelationProvider.scala:52)\n",
       "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:96)\n",
       "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:49)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:82)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:79)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:91)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:305)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:429)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:258)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:861)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure\n",
       "\n",
       "The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.\n",
       "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
       "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
       "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
       "\tat com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:61)\n",
       "\tat com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:105)\n",
       "\tat com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:151)\n",
       "\tat com.mysql.cj.exceptions.ExceptionFactory.createCommunicationsException(ExceptionFactory.java:167)\n",
       "\tat com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:89)\n",
       "\tat com.mysql.cj.NativeSession.connect(NativeSession.java:144)\n",
       "\tat com.mysql.cj.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:956)\n",
       "\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:826)\n",
       "\t... 60 more\n",
       "Caused by: java.net.SocketTimeoutException: connect timed out\n",
       "\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n",
       "\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n",
       "\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n",
       "\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n",
       "\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n",
       "\tat java.net.Socket.connect(Socket.java:613)\n",
       "\tat com.mysql.cj.protocol.StandardSocketFactory.connect(StandardSocketFactory.java:155)\n",
       "\tat com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:63)\n",
       "\t... 63 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-375575487821409>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#21\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mprofession_count_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjdbc\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mjdbc:mysql://127.0.0.1/hackathon\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mtable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mT_profession_count\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mproperties\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mroot\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpassword\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mRoot123$\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdriver\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcom.mysql.cj.jdbc.Driver\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mqueryTimeout\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m30\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtruncate\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrue\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124;03mCREATE TABLE T_profession_count (\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;03m  seqno int DEFAULT NULL,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124;03m  state_description varchar(255) DEFAULT NULL\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124;03m)'''\u001B[39;00m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1915\u001B[0m, in \u001B[0;36mDataFrameWriter.jdbc\u001B[0;34m(self, url, table, mode, properties)\u001B[0m\n\u001B[1;32m   1913\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m properties:\n\u001B[1;32m   1914\u001B[0m     jprop\u001B[38;5;241m.\u001B[39msetProperty(k, properties[k])\n\u001B[0;32m-> 1915\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjdbc\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjprop\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o1216.jdbc.\n: com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure\n\nThe last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createCommunicationsException(SQLError.java:174)\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:64)\n\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:836)\n\tat com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:456)\n\tat com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:246)\n\tat com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:198)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)\n\tat org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:102)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1(JdbcDialects.scala:123)\n\tat org.apache.spark.sql.jdbc.JdbcDialect.$anonfun$createConnectionFactory$1$adapted(JdbcDialects.scala:119)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.saveTableToJDBC(JdbcRelationProvider.scala:52)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:96)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:49)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:79)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:305)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:964)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:429)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:396)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:258)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:861)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.mysql.cj.exceptions.CJCommunicationsException: Communications link failure\n\nThe last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:61)\n\tat com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:105)\n\tat com.mysql.cj.exceptions.ExceptionFactory.createException(ExceptionFactory.java:151)\n\tat com.mysql.cj.exceptions.ExceptionFactory.createCommunicationsException(ExceptionFactory.java:167)\n\tat com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:89)\n\tat com.mysql.cj.NativeSession.connect(NativeSession.java:144)\n\tat com.mysql.cj.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:956)\n\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:826)\n\t... 60 more\nCaused by: java.net.SocketTimeoutException: connect timed out\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:613)\n\tat com.mysql.cj.protocol.StandardSocketFactory.connect(StandardSocketFactory.java:155)\n\tat com.mysql.cj.protocol.a.NativeSocketConnection.connect(NativeSocketConnection.java:63)\n\t... 63 more\n",
       "errorSummary": "com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#21\n",
    "\n",
    "#profession_count_df.write.jdbc(url=\"jdbc:mysql://127.0.0.1:3306/hackathon\",table=\"T_profession_count\",mode=\"overwrite\",properties={\"user\":\"root\",\"password\":\"Root123$\",\"driver\":\"com.mysql.cj.jdbc.Driver\",\"queryTimeout\":\"30\",\"truncate\":\"true\"})\n",
    "\n",
    "'''\n",
    "CREATE TABLE T_profession_count (\n",
    "  seqno int DEFAULT NULL,\n",
    "  profession varchar(255) DEFAULT NULL,\n",
    "  rofession_count int DEFAULT NULL,\n",
    "  average_age double DEFAULT NULL,\n",
    "  state_description varchar(255) DEFAULT NULL\n",
    ")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c7cbef0-0183-48b5-ac10-66c8b25d0972",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#22\n",
    "print(\"22. Spark Submit command: spark-submit --driver-memory 512M --num-executors 4 --executor-memory 1G --executor-cores 2 /home/hduser/install/part_a.py\")\n",
    "#spark-submit --driver-memory 512M --num-executors 4 --executor-memory 1G --executor-cores 2 /home/hduser/install/part_a.py"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Part B (1&2)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
